{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d530870",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Use \"pip install ____\" in the terminal to install missing libraries\n",
    "# Partition the libraries into smaller cells to avoid longer wait times\n",
    "\n",
    "# Import the below libraries:\n",
    "# For working with disk data \n",
    "import os # To create reproducible file paths\n",
    "import pickle # To serialize and reserialize objects (save and load objects from disk)\n",
    "\n",
    "# For visual displays within VS Code\n",
    "import warnings # To display warnings\n",
    "\n",
    "# For the progress bar\n",
    "from IPython.display import display\n",
    "from ipywidgets import IntProgress\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# For lazy loading\n",
    "import dask\n",
    "import gc\n",
    "\n",
    "# For API access\n",
    "import earthaccess # To access satellite imagery through the NASA API\n",
    "\n",
    "# To work with different types of data and conduct data analysis\n",
    "import cartopy.crs as ccrs # To project coordinates for spatial data and mapping\n",
    "import earthpy as et # For spatial data analysis\n",
    "import geopandas as gpd # To handle vectors/shapefiles\n",
    "import geoviews as gv # To process visualizations\n",
    "import numpy as np # To work with arrays\n",
    "import pandas as pd # To work with tables\n",
    "import re # For regular expressions; regular expression matching operations\n",
    "import rioxarray as rxr # To work with raster data\n",
    "import rioxarray.merge as rxrmerge # To merge rasters\n",
    "from shapely.geometry import Polygon # To work with polygons\n",
    "from sklearn.cluster import KMeans # For k-means clustering\n",
    "import xarray as xr # To work with data arrays\n",
    "\n",
    "# For hvPlot visualization\n",
    "import hvplot.pandas # To generate plots from Pandas dataframes\n",
    "import hvplot.xarray # To enable visualization of xarray objects\n",
    "\n",
    "# Set the GDAL parameters to avoid interruptions\n",
    "os.environ[\"GDAL_HTTP_MAX_RETRY\"] = \"5\"\n",
    "os.environ[\"GDAL_HTTP_RETRY_DELAY\"] = \"1\"\n",
    "\n",
    "# Hide non-critical warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd2b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A decorator modifies or extends another function\n",
    "# Pickling the function results ensures the function only runs if the output does not exist\n",
    " \n",
    "# Make the caching decorator:\n",
    "# Define the decorator\n",
    "# Set up the rules for the caching; base name for caching = \"func_key\"\n",
    "# Set the override to false to avoid reruns\n",
    "def cached(func_key, override=False):\n",
    "    # Dox strings explain what the function does\n",
    "    # Establish the parameters for the function\n",
    "    \"\"\"\n",
    "    A decorator to cache function results\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    key: str\n",
    "      File basename used to save pickled results\n",
    "    override: bool\n",
    "      When True, re-compute even if the results are already stored\n",
    "    \"\"\"\n",
    "    # Create the function that computes and saves or loads the results\n",
    "    def compute_and_cache_decorator(compute_function):\n",
    "        \"\"\"\n",
    "        Wrap the caching function\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        compute_function: function\n",
    "          The function to run and cache results\n",
    "        \"\"\"\n",
    "        # Create a function that accepts positional arguments\n",
    "        # args = Arguments that are defined by their name in the function call\n",
    "        def compute_and_cache(*args, **kwargs):\n",
    "            \"\"\"\n",
    "            Perform a computation and cache, or load cached result.\n",
    "            \n",
    "            Parameters\n",
    "            ==========\n",
    "            args\n",
    "              Positional arguments for the compute function\n",
    "            kwargs\n",
    "              Keyword arguments for the compute function\n",
    "            \"\"\"\n",
    "            # Add an identifier (\"cache_key\") from the particular function call\n",
    "            # If pass, build a single string by joining func_key with kwargs\n",
    "            # If no cache_key value is given, provide base name only\n",
    "            if 'cache_key' in kwargs:\n",
    "                key = '_'.join((func_key, kwargs['cache_key']))\n",
    "            else:\n",
    "                key = func_key\n",
    "\n",
    "            # Define a file path based on the directory structure in earthpy\n",
    "            path = os.path.join(\n",
    "                \n",
    "                # Establish the earthpy directory\n",
    "                et.io.HOME, \n",
    "                \n",
    "                # Establish the earthpy dataset\n",
    "                et.io.DATA_NAME, \n",
    "                \n",
    "                # Make a subdirectory called \"jars\"\n",
    "                'jars', \n",
    "                \n",
    "                # Use f-string (formatted string) to create a string by embedding the value of the variable \"key\" into the string \n",
    "                # Use .pickle file extension (\n",
    "                # Pickle file is a serialized python object\n",
    "                f'{key}.pickle')\n",
    "            \n",
    "            # Check if the cache exists already or if caching should be overriden\n",
    "            if not os.path.exists(path) or override:\n",
    "                \n",
    "                # Make a jars directory if needed\n",
    "                os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "                \n",
    "                # Run the compute function as the user did\n",
    "                result = compute_function(*args, **kwargs)\n",
    "                \n",
    "                # Pickle the object (save to file)\n",
    "                # Open the cache file at file name\n",
    "                with open(path, 'wb') as file:\n",
    "                    \n",
    "                    # Save the result without needing to recompute when loading it back into Python\n",
    "                    pickle.dump(result, file)\n",
    "            \n",
    "            ### If the file already exists/the cache is not being overriden:\n",
    "            else:\n",
    "               \n",
    "                # Unpickle the object (load the cached result)\n",
    "                with open(path, 'rb') as file:\n",
    "                    \n",
    "                    # Use pickle.load to unserialize the file back into a Python object\n",
    "                    result = pickle.load(file)\n",
    "\n",
    "            # Return either the computed result or cached result    \n",
    "            return result\n",
    "        \n",
    "        # Return wrapper function\n",
    "        return compute_and_cache\n",
    "    \n",
    "    # Return decorator configured by func_key and override\n",
    "    return compute_and_cache_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A level 12 watershed is the most detailed, granular way of dividing the watershed dataset\n",
    "# Assign the hydrologic unit code\n",
    "HUC_LEVEL = 12\n",
    "\n",
    "# Using the cache decorator, download, unzip, and read the shapefile:\n",
    "# Use @ to call the decorator\n",
    "# The Colorado watershed is in region 12\n",
    "@cached(f'wbd_14_hu{HUC_LEVEL}_gdf')\n",
    "\n",
    "# Make a function to read the file\n",
    "def read_wbd_file(wbd_filename, cache_key):\n",
    "    # Define the URL data is being pulled from\n",
    "    wbd_url = (\n",
    "        \"https://prd-tnm.s3.amazonaws.com/StagedProducts/Hydrography/WBD/HU2/Shape/\"\n",
    "        # Insert the name of the desired file\n",
    "        f\"{wbd_filename}.zip\"\n",
    "    )\n",
    "\n",
    "    # Download the data and unzip it into the directory\n",
    "    wbd_dir = et.data.get_data(url = wbd_url)\n",
    "\n",
    "    # Create a path to the shapefile in the directory\n",
    "    wbd_path = os.path.join(wbd_dir,\n",
    "                            'Shape',                            \n",
    "                            f'WBDHU{HUC_LEVEL}.shp')\n",
    "    \n",
    "    # Read the shapefile as a GeoDataFrame (GDF)\n",
    "    wbd_gdf = gpd.read_file(wbd_path,    \n",
    "                            # Use the pyogrio library\n",
    "                            engine = 'pyogrio')\n",
    "    \n",
    "    # Return the GDF for the watershed boundaries\n",
    "    return wbd_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the shapefile using the created wbd_read_file\n",
    "wbd_gdf = read_wbd_file(\"WBD_14_HU2_SHAPE\",\n",
    "                            f'hu{HUC_LEVEL}')\n",
    "\n",
    "# Open the shapefile to see the results\n",
    "wbd_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter/restrict the shapefile to the watershed being used\n",
    "# Define the GDF for the watershed by subsetting the GDF for the entire dataset\n",
    "delta_gdf = wbd_gdf[wbd_gdf[\n",
    "                            # Filter the GDF to the rows of the selected watershed\n",
    "                            # Hydrologic unit code (HUC) for the Gunnison watershed is 14020002\n",
    "                            # Dissolve to exclude individual rows\n",
    "                            f'huc{HUC_LEVEL}'].isin(['14020002'])].dissolve()\n",
    "\n",
    "# Display the data\n",
    "delta_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426afca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a site map with satellite imagery in the background \n",
    "(\n",
    "    # Project the delta's GDF to Mercator\n",
    "    delta_gdf.to_crs(ccrs.Mercator())\n",
    "\n",
    "    # Use hvPlot\n",
    "    .hvplot(\n",
    "        # Make the watershed transparent\n",
    "        alpha = 0.35, fill_color = \"white\",\n",
    "        # Add the satellite basemap\n",
    "        tiles = \"EsriImagery\",\n",
    "        # Plot in Mercator\n",
    "        crs = ccrs.Mercator())\n",
    "        \n",
    "    # Set the plot size\n",
    "    .opts(width = 600, height = 300)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45bf823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log into EarthAccess\n",
    "earthaccess.login(persist = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for wanted HLS granules \n",
    "results = earthaccess.search_data(\n",
    "    # Specify the desired dataset and spatial resolution\n",
    "    short_name = \"HLSL30\",\n",
    "    # Specify that cloud data is being used\n",
    "    cloud_hosted = True,\n",
    "    # Use the bounding box to establish the watershed boundary\n",
    "    bounding_box = tuple(delta_gdf.total_bounds),\n",
    "    # Set the temporal range for the data\n",
    "    temporal = (\"2024-06\", \"2024-08\")\n",
    ")\n",
    "\n",
    "# View the results\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b5d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to process all the granules and extract information for each granule from the Earth Access search:\n",
    "# Define the function\n",
    "def get_earthaccess_links(results):\n",
    "    # Make and display a progress bar\n",
    "    f = IntProgress(min = 0, max = len(results), description = \"Open granules\")\n",
    "    display(f) # To display the progress bar\n",
    "\n",
    "    # Use a regular expression to extract tile_id and bank from THE .tif files\n",
    "    re_url = re.compile(\n",
    "        r'\\.(?P<tile_id>T\\w+)\\.\\d+T\\d+\\.v\\d+\\.\\d+\\.(?P<band>[A-Za-z0-9]+)\\.tif$'\n",
    "    )\n",
    "\n",
    "    # Accumulate GDF rows from each granule\n",
    "    link_rows = []\n",
    "\n",
    "    # Loop over the granules to extract info\n",
    "    for granule in results:\n",
    "        # Locate the metadata (Universal metadata model = UMM)\n",
    "        info_dict = granule['umm']\n",
    "        # Pull out the unique identifier for the granule\n",
    "        granule_id = info_dict['GranuleUR']\n",
    "        # Extract the date and time \n",
    "        datetime = pd.to_datetime(\n",
    "            info_dict['TemporalExtent']['RangeDateTime']['BeginningDateTime']\n",
    "        )\n",
    "        # Extact the boundary coordinates for the granule\n",
    "        points = (\n",
    "            info_dict\n",
    "            ['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['GPolygons'][0]['Boundary']['Points']\n",
    "        )\n",
    "        # Create a polygon using coordinate points for the granules\n",
    "        geometry = Polygon(\n",
    "            [(point['Longitude'],\n",
    "            point['Latitude']) for point in points]\n",
    "        )\n",
    "\n",
    "        # Get the URL and open granule\n",
    "        files = earthaccess.open([granule])\n",
    "\n",
    "        # Loop through each file in the granule\n",
    "        for file in files:\n",
    "            # Use a URL regular expression to get the URL\n",
    "            match = re_url.search(file.full_name) \n",
    "            # If a match is found, append data to initialized link_rows GDF \n",
    "            if match is not None:\n",
    "                link_rows.append(\n",
    "                    # Make a GDF with the granule's data and geometry\n",
    "                    gpd.GeoDataFrame(\n",
    "                        dict(\n",
    "                            # For the timestamp\n",
    "                            datetime = [datetime],\n",
    "                            # For a unique ID\n",
    "                            tile_id = [match.group('tile_id')],\n",
    "                            # For the band name\n",
    "                            band = [match.group('band')],\n",
    "                            # For the URL\n",
    "                            url = [file], \n",
    "                            # For the polygon\n",
    "                            geometry = [geometry],\n",
    "                            # For the coordinate reference system (CRS)\n",
    "                            crs = \"ESPG:4326\"\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Update the progress bar after each granule is done\n",
    "        f.value+=1\n",
    "\n",
    "    # Combine into a single GDF\n",
    "    file_df = pd.concat(link_rows).reset_index(drop = True)   \n",
    "\n",
    "    # Return the final GDF file\n",
    "    return file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the granule results\n",
    "granule = results[0]\n",
    "granule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_dict = granule ['umm']\n",
    "info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e4dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to get the granule search results\n",
    "\"\"\" The library ipywidgets needs to be installed to display the IntProgress bar. This can be done by using the\n",
    "command \"pip install ipywidgets\" in the terminal. \"from IPython.display import display\" and \"from ipywidgets \n",
    "import IntProgress\" are also used in the first cell to help Python process this display. \"\"\"\n",
    "file_df = get_earthaccess_links(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cached decorator to the function\n",
    "@cached('delta_reflecance_da_df')\n",
    "\n",
    "# Write a function that computes reflectance data using the search results (dataframe of URLs) and watershed boundary\n",
    "def compute_reflectance_da(search_results, boundary_gdf):\n",
    "    \"\"\"\n",
    "    Use VSI to connect to the files, crop them, apply a cloud mask, and wrangle the data\n",
    "    Return a single reflectance dataframe with bands as columns and the centroid coordinates and datetime as the\n",
    "    index\n",
    "\n",
    "    Parameters\n",
    "    search_results: list # The search result links to the files (URLs)\n",
    "    boundary_gdf: gpd.GeoDataFrame # The boundary is used to crop the data\n",
    "    \"\"\"\n",
    "    # Write a function to open the raster from the URL, apply scale factor, and crop and mask data\n",
    "    def open_dataarray(url, boundary_proj_gdf, scale = 1, masked = True):\n",
    "        # Open the raster data\n",
    "        da = rxr.open_rasterio(url, masked = masked, \n",
    "                               # Add chunking\n",
    "                               chunks={\"x\": 1024, \"y\": 1024}\n",
    "                               ).squeeze() * scale\n",
    "        # If needed, reproject the boundary to match the raster's CRS\n",
    "        if boundary_proj_gdf is None:\n",
    "            boundary_proj_gdf = boundary_gdf.to_crs(da.rio.crs)\n",
    "        # Crop the raster to the bounding box\n",
    "        cropped = da.rio.clip_box(*boundary_proj_gdf.total_bounds)\n",
    "        # Return the cropped raster\n",
    "        return cropped\n",
    "\n",
    "    # Write a function to apply a cloud mask\n",
    "    # Mask out law quality data by bit\n",
    "    def compute_quality_masked(da, mask_bits = [1,2,3]):\n",
    "        # Unpack the bits to a new axis\n",
    "        bits = (\n",
    "            # Unpack each number into individual bits\n",
    "            np.unpackbits(\n",
    "                # Convert to 8-bit unsigned integer\n",
    "                da.astype(np.uint8),\n",
    "                # Set the order of the bits\n",
    "                bitorder = 'little'\n",
    "            # Reshape to match the original data with an extra dimension for the bits\n",
    "            ).reshape(da.shape + (-1,))\n",
    "        )\n",
    "        # Grab the bits and check if they are flagged\n",
    "        mask = np.prod(\n",
    "            bits[\n",
    "                ..., mask_bits] == 0, \n",
    "                     axis = -1\n",
    "        )\n",
    "        # Return the mask\n",
    "        return mask\n",
    "\n",
    "    # Grab the metadata\n",
    "    file_df = get_earthaccess_links(search_results)\n",
    "\n",
    "    # Store results for each granule in a list\n",
    "    granule_da_rows = []\n",
    "\n",
    "    # Store the projected boundary\n",
    "    boundary_proj_gdf = None\n",
    "\n",
    "    # Group the data for each granule\n",
    "    group_iter = file_df.groupby(\n",
    "        # By datetime and tile_id\n",
    "        ['datetime', 'tile_id']\n",
    "    )\n",
    "\n",
    "    # Loop through each image and its metadata\n",
    "    for (datetime, tile_id), granule_df in tqdm(group_iter):\n",
    "        # Print the status bar\n",
    "        print(f'Processing granule {tile_id} {datetime}')\n",
    "        # Find each granule's cloud mask (fmask) URL\n",
    "        cloud_mask_url = (\n",
    "            granule_df.loc[granule_df.band == 'Fmask', 'url']\n",
    "            .values[0])\n",
    "        # Open the granule cloud mask\n",
    "        cloud_masked_cropped_da = open_dataarray(cloud_mask_url, boundary_proj_gdf, masked = False)\n",
    "        # Compute the cloud mask\n",
    "        cloud_mask = compute_quality_masked(cloud_masked_cropped_da)\n",
    "        # Free memory early\n",
    "        del cloud_masked_cropped_da        \n",
    "        gc.collect()\n",
    "        # Loop through each spectral band to open, crop, and mask the band\n",
    "        da_list = []\n",
    "        df_list = []\n",
    "\n",
    "        # Loop through each band in the granule\n",
    "        for i, row in granule_df.iterrows():\n",
    "            # Only loop through the spectral bands\n",
    "            if row.band.startswith(('B')):\n",
    "                # Open the band's raster and scale to reflectance\n",
    "                band_cropped = open_dataarray(\n",
    "                    row.url, \n",
    "                    boundary_proj_gdf, \n",
    "                    scale = 0.0001\n",
    "                )\n",
    "                # Name the raster by the band\n",
    "                band_cropped.name = row.band\n",
    "                # Cut raster memory in half\n",
    "                band_cropped = band_cropped.astype(\"float16\")\n",
    "                # Apply the cloud mask to the raster\n",
    "                row['da'] = band_cropped.where(cloud_mask)\n",
    "                # Append the row to granule_da_rows\n",
    "                granule_da_rows.append(row.to_frame().T)\n",
    "                \n",
    "    # Reassemble the metadata dataframe\n",
    "    return pd.concat(granule_da_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ac15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "reflectance_da_df = compute_reflectance_da(results, delta_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataframe\n",
    "reflectance_da_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e0d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cache decorator\n",
    "@cached('delta_reflectance_da')\n",
    "\n",
    "# Create a function to merge and composite reflectance data from multiple granules\n",
    "\"\"\"The end result will be a single composite reflectance image for each spectral band.\"\"\"\n",
    "def merge_and_composite_arrays(granule_da_df):\n",
    "    # Initialize a list to store composites after processing\n",
    "    da_list = []\n",
    "\n",
    "    # Loop over each spectral band\n",
    "    for band, band_df in granule_da_df.groupby('band'):\n",
    "        # Create a list for storing merged data arrays (one per date)\n",
    "        merged_das = []\n",
    "        # Loop over the dates and times of the image acquisition and merge the granules for each data\n",
    "        for datetime, date_df in band_df.groupby('datetime'):\n",
    "            # Merge granules for each date\n",
    "            merged_da = rxrmerge.merge_arrays(list(date_df))\n",
    "            # Mask negative values (could be no data or invalid data)\n",
    "            merged_da = merged_da.where(merged_da > 0)\n",
    "            # Append to the initialized merged_das list\n",
    "            merged_das.append(merged_da)\n",
    "        # Composite images across dates\n",
    "        composite_da = xr.concat(merged_das,\n",
    "                                 # Make a datetime dimension\n",
    "                                 # Calculate the median value across datetimes for the pixels\n",
    "                                 dim = 'datetime').median('datetime')\n",
    "        # Assign band number to the attribute of the composite data array\n",
    "        composite_da['band'] = int(band[1:])\n",
    "        # Name the composite data array\n",
    "        composte_da.name = 'reflectance'\n",
    "        # Add the processed and composite data array to the list\n",
    "        da_list.append(composite_da)\n",
    "    # Concatenate the composite data arrays for each band along the band dimension\n",
    "    return xr.concat(da_list, dim = 'band')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to get the final composite reflectance data \n",
    "reflectance_da = merge_and_composite_arrays(reflectance_da_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the spectral DataArray to a tidy DataFrame\n",
    "model_df = (reflectance_da,\n",
    "            # Flatten the array into a long dataframe\n",
    "            .to_dataframe()\n",
    "            # Select the reflectance column\n",
    "            .reflectance\n",
    "            # Make the column wide;\n",
    "            \"\"\"Each row will be a pixel location. Each column is a spectral band with the reflectance value.\"\"\"\n",
    "            .unstack('band')\n",
    ")\n",
    "\n",
    "# Filter out rows with no data\n",
    "model_df = model_df.drop(columns = [10, 11]).dropna()\n",
    "\n",
    "# Display the dataframe\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the minimum and maximum values\n",
    "min_values = model_df.min()\n",
    "max_values = model_df.max()\n",
    "\n",
    "# Print the minimum and maximum values\n",
    "print(min_values)\n",
    "print(max_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad651217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the k-means model \n",
    "k_means = KMeans(n_clusters = 5)\n",
    "\n",
    "# Fit the model and predict\n",
    "prediction = k_means.fit_predict(model_df.values)\n",
    "\n",
    "# Add the predicted values back to the model dataframe\n",
    "model_df['clusters'] = prediction\n",
    "\n",
    "# Display the dataframe\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6876e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data array with bands to use for red, green, and blue (rgb)\n",
    "rgb = reflectance_da.sel(band = [4, 3, 2])\n",
    "\n",
    "# Plot rgb\n",
    "(\n",
    "    rgb_plot.hvplot.rgb(y = 'y',\n",
    "                    x = 'x',\n",
    "                    bands = 'band',\n",
    "                    xaxis = None,\n",
    "                    yaxis = None)\n",
    "\n",
    "                    model_df.clusters.to_xarray().sortby(['x', 'y']).hvplot(\n",
    "        cmap=\"Colorblind\", aspect='equal') \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0461b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earth-analytics-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
